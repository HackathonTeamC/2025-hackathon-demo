# ã‚¿ã‚¹ã‚¯7: ä¼šè©±å±¥æ­´åˆ†æ

## æ¦‚è¦
éå»ã®Slackä¼šè©±ã‚’åˆ†æã—ã€èˆˆå‘³æ·±ã„è©±é¡Œã‚’æŠ½å‡ºã—ã¦å®šæœŸæŠ•ç¨¿ã«æ´»ç”¨ã—ã¾ã™ã€‚

## ç›®çš„
- ãƒãƒ£ãƒ³ãƒãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®å–å¾—
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨è©±é¡Œåˆ†æ
- äººæ°—ã®ã‚ã£ãŸè©±é¡Œã®å†æèµ·

## å®Ÿè£…å†…å®¹

### 1. ä¼šè©±å±¥æ­´å–å¾—Lambda

#### 1.1 conversation_analyzer/handler.py

```python
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List
import boto3
import sys
sys.path.append('/opt/python')

from slack_client import SlackClient
from database import ConversationsDB
from text_analyzer import TextAnalyzer

CONVERSATIONS_TABLE = os.environ['CONVERSATIONS_TABLE']
TARGET_CHANNELS = os.environ.get('TARGET_CHANNELS', '').split(',')  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
ANALYSIS_DAYS = int(os.environ.get('ANALYSIS_DAYS', '7'))  # éå»7æ—¥åˆ†

def lambda_handler(event, context):
    """ä¼šè©±å±¥æ­´åˆ†æLambdaã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    
    å®šæœŸçš„ã«å®Ÿè¡Œï¼ˆä¾‹ï¼šæ¯æ—¥æ·±å¤œï¼‰ã—ã¦ä¼šè©±ã‚’åˆ†æ
    """
    print(f"Conversation analysis started at {datetime.now()}")
    
    try:
        slack = SlackClient()
        conversations_db = ConversationsDB(CONVERSATIONS_TABLE)
        analyzer = TextAnalyzer()
        
        # åˆ†æå¯¾è±¡æœŸé–“
        since = datetime.now() - timedelta(days=ANALYSIS_DAYS)
        oldest_ts = str(since.timestamp())
        
        analyzed_count = 0
        
        for channel_id in TARGET_CHANNELS:
            if not channel_id:
                continue
            
            print(f"Analyzing channel: {channel_id}")
            
            # ãƒãƒ£ãƒ³ãƒãƒ«å±¥æ­´ã‚’å–å¾—
            messages = slack.get_channel_history(
                channel=channel_id,
                limit=1000,
                oldest=oldest_ts
            )
            
            # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†æ
            for message in messages:
                # botãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
                if message.get('subtype') == 'bot_message':
                    continue
                
                # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸€å®šæ•°ä»¥ä¸Šã®ã‚‚ã®ã‚’æŠ½å‡º
                reactions = message.get('reactions', [])
                reaction_count = sum([r['count'] for r in reactions])
                
                if reaction_count < 3:  # é–¾å€¤
                    continue
                
                # ã‚¹ãƒ¬ãƒƒãƒ‰ã®è¿”ä¿¡æ•°ã‚’å–å¾—
                reply_count = message.get('reply_count', 0)
                
                # ä¼šè©±ãŒç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã‹åˆ¤å®š
                if reaction_count >= 5 or reply_count >= 3:
                    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
                    text = message.get('text', '')
                    keywords = analyzer.extract_keywords(text)
                    sentiment = analyzer.analyze_sentiment(text)
                    
                    # å‚åŠ è€…ã‚’å–å¾—
                    participants = []
                    if 'user' in message:
                        participants.append(message['user'])
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    conversation_data = {
                        'channel_id': channel_id,
                        'message_ts': message['ts'],
                        'keywords': keywords,
                        'participants': participants,
                        'reaction_count': reaction_count,
                        'comment_count': reply_count,
                        'sentiment': sentiment,
                        'is_used_for_topic': False,
                        'created_at': datetime.fromtimestamp(float(message['ts'])).isoformat(),
                        'analyzed_at': datetime.now().isoformat()
                    }
                    
                    conversations_db.save_conversation(conversation_data)
                    analyzed_count += 1
        
        print(f"Analysis completed: {analyzed_count} conversations saved")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Analysis completed',
                'analyzed_count': analyzed_count
            })
        }
    
    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
```

#### 1.2 text_analyzer.pyï¼ˆsrc/shared/text_analyzer.pyï¼‰

```python
import re
from typing import List
from collections import Counter

class TextAnalyzer:
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    # æ—¥æœ¬èªãƒ»è‹±èªã®ä¸€èˆ¬çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
    STOP_WORDS = {
        'ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã“ã®', 'ãã®',
        'the', 'is', 'are', 'was', 'were', 'a', 'an', 'to'
    }
    
    def extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            top_n: æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
        
        Returns:
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        # URLã‚’é™¤å»
        text = re.sub(r'http\S+', '', text)
        
        # è¨˜å·ã‚’é™¤å»
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # å˜èªã«åˆ†å‰²ï¼ˆæ—¥æœ¬èªã‚‚å¯¾å¿œã™ã‚‹ãŸã‚ã‚¹ãƒšãƒ¼ã‚¹åˆ†å‰²ï¼‰
        words = text.lower().split()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»
        words = [w for w in words if w not in self.STOP_WORDS and len(w) > 2]
        
        # å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        word_counts = Counter(words)
        
        # ä¸Šä½Nä»¶ã‚’è¿”ã™
        top_words = [word for word, count in word_counts.most_common(top_n)]
        
        return top_words
    
    def analyze_sentiment(self, text: str) -> str:
        """æ„Ÿæƒ…åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            'positive' | 'neutral' | 'negative'
        """
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå˜èªã®ãƒªã‚¹ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        positive_words = [
            'è‰¯ã„', 'ã„ã„', 'æœ€é«˜', 'ç´ æ™´ã‚‰ã—ã„', 'æ¥½ã—ã„', 'å¬‰ã—ã„',
            'good', 'great', 'awesome', 'excellent', 'happy', 'ğŸ‘', 'â¤ï¸', 'ğŸ‰'
        ]
        
        negative_words = [
            'æ‚ªã„', 'ãƒ€ãƒ¡', 'é›£ã—ã„', 'å›°ã‚‹', 'å¤±æ•—',
            'bad', 'difficult', 'problem', 'issue', 'ğŸ˜¢', 'ğŸ˜'
        ]
        
        text_lower = text.lower()
        
        positive_count = sum([1 for word in positive_words if word in text_lower])
        negative_count = sum([1 for word in negative_words if word in text_lower])
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
```

#### 1.3 database.pyï¼ˆConversationsDBè¿½åŠ ï¼‰

```python
class ConversationsDB:
    """ä¼šè©±å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, table_name: str):
        dynamodb = boto3.resource('dynamodb')
        self.table = dynamodb.Table(table_name)
    
    def save_conversation(self, conversation_data: Dict) -> str:
        """ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        Args:
            conversation_data: ä¼šè©±ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            conversation_id
        """
        import uuid
        conversation_id = str(uuid.uuid4())
        item = {
            'conversation_id': conversation_id,
            **conversation_data
        }
        self.table.put_item(Item=item)
        return conversation_id
    
    def get_popular_conversations(
        self,
        channel_id: str,
        days: int = 7,
        limit: int = 10
    ) -> List[Dict]:
        """äººæ°—ã®ã‚ã£ãŸä¼šè©±ã‚’å–å¾—
        
        Args:
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
            days: éå»ä½•æ—¥åˆ†
            limit: å–å¾—æ•°
        
        Returns:
            ä¼šè©±ã®ãƒªã‚¹ãƒˆ
        """
        since = (datetime.now() - timedelta(days=days)).isoformat()
        
        response = self.table.query(
            IndexName='PopularityIndex',
            KeyConditionExpression=Key('channel_id').eq(channel_id),
            ScanIndexForward=False,  # é™é †
            Limit=limit
        )
        
        return response.get('Items', [])
    
    def get_unused_conversations(self, channel_id: str) -> List[Dict]:
        """ã¾ã è©±é¡Œã¨ã—ã¦ä½¿ã£ã¦ã„ãªã„ä¼šè©±ã‚’å–å¾—
        
        Args:
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
        
        Returns:
            ä¼šè©±ã®ãƒªã‚¹ãƒˆ
        """
        response = self.table.query(
            IndexName='ChannelTimeIndex',
            KeyConditionExpression=Key('channel_id').eq(channel_id),
            FilterExpression='is_used_for_topic = :false',
            ExpressionAttributeValues={':false': False},
            ScanIndexForward=False,
            Limit=20
        )
        
        return response.get('Items', [])
    
    def mark_as_used(self, conversation_id: str) -> None:
        """ä¼šè©±ã‚’ä½¿ç”¨æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
        
        Args:
            conversation_id: ä¼šè©±ID
        """
        self.table.update_item(
            Key={'conversation_id': conversation_id},
            UpdateExpression='SET is_used_for_topic = :true',
            ExpressionAttributeValues={':true': True}
        )
```

### 2. éå»ã®ä¼šè©±ãƒ™ãƒ¼ã‚¹è©±é¡Œç”Ÿæˆ

#### 2.1 scheduled_posterã®æ‹¡å¼µ

```python
def select_conversation_based_topic(
    conversations_db: ConversationsDB,
    slack: SlackClient,
    channel_id: str
) -> Optional[Dict]:
    """éå»ã®ä¼šè©±ã‹ã‚‰è©±é¡Œã‚’ç”Ÿæˆ
    
    Args:
        conversations_db: ConversationsDBã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        slack: SlackClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸè©±é¡Œæƒ…å ±
    """
    # æœªä½¿ç”¨ã®äººæ°—ä¼šè©±ã‚’å–å¾—
    conversations = conversations_db.get_unused_conversations(channel_id)
    
    if not conversations:
        return None
    
    # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
    conversations.sort(key=lambda x: x['reaction_count'], reverse=True)
    selected = conversations[0]
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æ–‡ã‚’ç”Ÿæˆ
    keywords = selected.get('keywords', [])
    keyword_text = 'ã€'.join(keywords[:3]) if keywords else 'ã‚ã®ãƒˆãƒ”ãƒƒã‚¯'
    
    # å…ƒã®ç™ºè¨€è€…ã‚’å–å¾—
    participants = selected.get('participants', [])
    mention = f"<@{participants[0]}>" if participants else 'ãƒ¡ãƒ³ãƒãƒ¼'
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
    message_ts = selected['message_ts'].replace('.', '')
    message_link = f"https://slack.com/archives/{channel_id}/p{message_ts}"
    
    topic_text = (
        f"ğŸ’¡ å…ˆé€±{mention}ã•ã‚“ãŒè©±ã—ã¦ã„ãŸã€Œ{keyword_text}ã€ã«ã¤ã„ã¦ã€\n"
        f"ã‚‚ã£ã¨è©³ã—ãèããŸã„æ–¹ã¯ã„ã¾ã™ã‹ï¼Ÿ\n"
        f"<{message_link}|å…ƒã®ä¼šè©±ã¯ã“ã¡ã‚‰>"
    )
    
    # ä½¿ç”¨æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
    conversations_db.mark_as_used(selected['conversation_id'])
    
    return {
        'content': topic_text,
        'conversation_id': selected['conversation_id'],
        'original_message_ts': selected['message_ts']
    }
```

### 3. EventBridgeè¨­å®šï¼ˆä¼šè©±åˆ†æï¼‰

**Terraformè¿½åŠ :**
```hcl
# æ¯æ—¥æ·±å¤œ2:00ã«ä¼šè©±åˆ†æã‚’å®Ÿè¡Œ
resource "aws_cloudwatch_event_rule" "conversation_analysis" {
  name                = "${var.project_name}-conversation-analysis"
  description         = "Analyze conversations daily"
  schedule_expression = "cron(0 17 * * ? *)"  # UTC 17:00 = JST 2:00
  
  tags = local.common_tags
}

resource "aws_cloudwatch_event_target" "conversation_analysis_target" {
  rule      = aws_cloudwatch_event_rule.conversation_analysis.name
  target_id = "ConversationAnalyzerLambda"
  arn       = aws_lambda_function.conversation_analyzer.arn
}

resource "aws_lambda_permission" "allow_eventbridge_analyzer" {
  statement_id  = "AllowExecutionFromEventBridge"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.conversation_analyzer.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.conversation_analysis.arn
}
```

## æˆæœç‰©
- [ ] conversation_analyzer Lambdaå®Ÿè£…å®Œäº†
- [ ] text_analyzer.py å®Ÿè£…å®Œäº†
- [ ] ConversationsDBå®Ÿè£…å®Œäº†
- [ ] éå»ä¼šè©±ãƒ™ãƒ¼ã‚¹è©±é¡Œç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…å®Œäº†
- [ ] EventBridgeè¨­å®šå®Œäº†

## æ¤œè¨¼æ–¹æ³•

```python
# ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®ãƒ†ã‚¹ãƒˆ
from text_analyzer import TextAnalyzer

analyzer = TextAnalyzer()

text = "æœ€è¿‘Dockerã‚’å‹‰å¼·ã—ã¦ã„ã¦ã€ã‚³ãƒ³ãƒ†ãƒŠã®ä»•çµ„ã¿ãŒé¢ç™½ã„ã§ã™ï¼"
keywords = analyzer.extract_keywords(text)
print(f"Keywords: {keywords}")

sentiment = analyzer.analyze_sentiment(text)
print(f"Sentiment: {sentiment}")
```

## æ¬¡ã®ã‚¿ã‚¹ã‚¯
[ã‚¿ã‚¹ã‚¯8: ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¤œçŸ¥ãƒ»å‡¦ç†](./task-08-reaction-handler.md)

## å‚è€ƒè³‡æ–™
- [è‡ªç„¶è¨€èªå‡¦ç†å…¥é–€](https://www.nltk.org/)
- [å½¢æ…‹ç´ è§£æï¼ˆæ—¥æœ¬èªï¼‰](https://github.com/mocobeta/janome)
